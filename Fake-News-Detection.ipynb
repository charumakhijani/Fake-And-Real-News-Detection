{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = pd.read_csv(\"True.csv\")\n",
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target based on Real and Fake data\n",
    "real['Category'] = 1\n",
    "fake['Category'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21417, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  Category  \n",
       "0  December 31, 2017          1  \n",
       "1  December 29, 2017          1  \n",
       "2  December 31, 2017          1  \n",
       "3  December 30, 2017          1  \n",
       "4  December 29, 2017          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(real.shape)\n",
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23481, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  Category  \n",
       "0  December 31, 2017         0  \n",
       "1  December 31, 2017         0  \n",
       "2  December 30, 2017         0  \n",
       "3  December 29, 2017         0  \n",
       "4  December 25, 2017         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fake.shape)\n",
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([real, fake]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  Category  \n",
       "0  December 31, 2017          1  \n",
       "1  December 29, 2017          1  \n",
       "2  December 31, 2017          1  \n",
       "3  December 30, 2017          1  \n",
       "4  December 29, 2017          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    As U.S. budget fight looms, Republicans flip t...\n",
       "1    U.S. military to accept transgender recruits o...\n",
       "2    Senior U.S. Republican senator: 'Let Mr. Muell...\n",
       "3    FBI Russia probe helped by Australian diplomat...\n",
       "4    Trump wants Postal Service to charge 'much mor...\n",
       "Name: final_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['final_text'] = dataset['title'] + dataset['text']\n",
    "dataset['final_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23481\n",
       "1    21417\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">0</th>\n",
       "      <th>Government News</th>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middle-east</th>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td>9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_News</th>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left-news</th>\n",
       "      <td>4459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>6841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>politicsNews</th>\n",
       "      <td>11272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldnews</th>\n",
       "      <td>10145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          final_text\n",
       "Category subject                    \n",
       "0        Government News        1570\n",
       "         Middle-east             778\n",
       "         News                   9050\n",
       "         US_News                 783\n",
       "         left-news              4459\n",
       "         politics               6841\n",
       "1        politicsNews          11272\n",
       "         worldnews             10145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['Category','subject','final_text']].groupby(['Category','subject']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Text to get Stemmed and Lemmatized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = []\n",
    "lemmatized_text = []\n",
    "final_text_result = []\n",
    "for text in dataset['final_text']:\n",
    "    result = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    result = result.lower()\n",
    "    result = result.split()\n",
    "    result = [r for r in result if r not in set(stopwords.words('english'))]\n",
    "    final_text_result.append(\" \".join(result))\n",
    "    stemmed_result = [porter_stemmer.stem(r) for r in result]\n",
    "    stemmed_text.append(\" \".join(stemmed_result))\n",
    "    lemmatized_result = [lemmatizer.lemmatize(r) for r in result]\n",
    "    lemmatized_text.append(\" \".join(lemmatized_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n",
      "44898\n",
      "44898\n"
     ]
    }
   ],
   "source": [
    "print(len(final_text_result))\n",
    "print(len(stemmed_text))\n",
    "print(len(lemmatized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models using CountVectorizer and TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
    "    pipe = Pipeline([('vector', vectorizer),\n",
    "                    ('model', classifier)])\n",
    "    model = pipe.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix: \\n\", cm)\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******USING STEMMED TEXT********\n",
      "\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.6\n",
      "Confusion Matrix: \n",
      " [[7043   26]\n",
      " [  28 6373]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 98.66\n",
      "Confusion Matrix: \n",
      " [[6963  106]\n",
      " [  74 6327]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.98      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.29\n",
      "Confusion Matrix: \n",
      " [[7010   59]\n",
      " [  37 6364]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.09\n",
      "Confusion Matrix: \n",
      " [[6998   71]\n",
      " [  51 6350]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 95.3\n",
      "Confusion Matrix: \n",
      " [[6756  313]\n",
      " [ 320 6081]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      7069\n",
      "           1       0.95      0.95      0.95      6401\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13470\n",
      "   macro avg       0.95      0.95      0.95     13470\n",
      "weighted avg       0.95      0.95      0.95     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 93.56\n",
      "Confusion Matrix: \n",
      " [[6728  341]\n",
      " [ 526 5875]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      7069\n",
      "           1       0.95      0.92      0.93      6401\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     13470\n",
      "   macro avg       0.94      0.93      0.94     13470\n",
      "weighted avg       0.94      0.94      0.94     13470\n",
      "\n",
      "\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 96.82\n",
      "Confusion Matrix: \n",
      " [[6845  224]\n",
      " [ 204 6197]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      7069\n",
      "           1       0.97      0.97      0.97      6401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     13470\n",
      "   macro avg       0.97      0.97      0.97     13470\n",
      "weighted avg       0.97      0.97      0.97     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 96.82\n",
      "Confusion Matrix: \n",
      " [[6845  224]\n",
      " [ 204 6197]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      7069\n",
      "           1       0.97      0.97      0.97      6401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     13470\n",
      "   macro avg       0.97      0.97      0.97     13470\n",
      "weighted avg       0.97      0.97      0.97     13470\n",
      "\n",
      "\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.52\n",
      "Confusion Matrix: \n",
      " [[7035   34]\n",
      " [  31 6370]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       0.99      1.00      0.99      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.47\n",
      "Confusion Matrix: \n",
      " [[7031   38]\n",
      " [  33 6368]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 78.25\n",
      "Confusion Matrix: \n",
      " [[5682 1387]\n",
      " [1543 4858]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80      7069\n",
      "           1       0.78      0.76      0.77      6401\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     13470\n",
      "   macro avg       0.78      0.78      0.78     13470\n",
      "weighted avg       0.78      0.78      0.78     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 86.82\n",
      "Confusion Matrix: \n",
      " [[5743 1326]\n",
      " [ 449 5952]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87      7069\n",
      "           1       0.82      0.93      0.87      6401\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     13470\n",
      "   macro avg       0.87      0.87      0.87     13470\n",
      "weighted avg       0.88      0.87      0.87     13470\n",
      "\n",
      "\n",
      "\n",
      " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.7\n",
      "Confusion Matrix: \n",
      " [[7058   11]\n",
      " [  30 6371]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.59\n",
      "Confusion Matrix: \n",
      " [[7052   17]\n",
      " [  38 6363]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7069\n",
      "           1       1.00      0.99      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "\n",
      "\n",
      " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.6\n",
      "Confusion Matrix: \n",
      " [[7027   42]\n",
      " [  12 6389]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.58\n",
      "Confusion Matrix: \n",
      " [[7029   40]\n",
      " [  16 6385]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 96.07\n",
      "Confusion Matrix: \n",
      " [[6935  134]\n",
      " [ 396 6005]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      7069\n",
      "           1       0.98      0.94      0.96      6401\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     13470\n",
      "   macro avg       0.96      0.96      0.96     13470\n",
      "weighted avg       0.96      0.96      0.96     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 96.32\n",
      "Confusion Matrix: \n",
      " [[6949  120]\n",
      " [ 376 6025]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      7069\n",
      "           1       0.98      0.94      0.96      6401\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     13470\n",
      "   macro avg       0.96      0.96      0.96     13470\n",
      "weighted avg       0.96      0.96      0.96     13470\n",
      "\n",
      "\n",
      "\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.65\n",
      "Confusion Matrix: \n",
      " [[7036   33]\n",
      " [  14 6387]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.65\n",
      "Confusion Matrix: \n",
      " [[7036   33]\n",
      " [  14 6387]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"******USING STEMMED TEXT********\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(stemmed_text, dataset['Category'], test_size = 0.3, random_state= 0)\n",
    "classifiers = [LogisticRegression(), SGDClassifier(), MultinomialNB(), BernoulliNB(), LinearSVC(),\n",
    "              KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n",
    "               RandomForestClassifier(), XGBClassifier()]\n",
    "for classifier in classifiers:\n",
    "    print(\"\\n\\n\", classifier)\n",
    "    print(\"***********Usng Count Vectorizer****************\")\n",
    "    get_prediction(CountVectorizer(), classifier, X_train, X_test, y_train, y_test)\n",
    "    print(\"***********Usng TFIDF Vectorizer****************\")\n",
    "    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******USING LEMMATIZED TEXT********\n",
      "\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.64\n",
      "Confusion Matrix: \n",
      " [[7046   23]\n",
      " [  26 6375]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 98.73\n",
      "Confusion Matrix: \n",
      " [[6966  103]\n",
      " [  68 6333]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.98      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.5\n",
      "Confusion Matrix: \n",
      " [[7026   43]\n",
      " [  24 6377]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      7069\n",
      "           1       0.99      1.00      0.99      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       0.99      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.14\n",
      "Confusion Matrix: \n",
      " [[6996   73]\n",
      " [  43 6358]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 95.46\n",
      "Confusion Matrix: \n",
      " [[6758  311]\n",
      " [ 301 6100]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7069\n",
      "           1       0.95      0.95      0.95      6401\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13470\n",
      "   macro avg       0.95      0.95      0.95     13470\n",
      "weighted avg       0.95      0.95      0.95     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 93.87\n",
      "Confusion Matrix: \n",
      " [[6725  344]\n",
      " [ 482 5919]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      7069\n",
      "           1       0.95      0.92      0.93      6401\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     13470\n",
      "   macro avg       0.94      0.94      0.94     13470\n",
      "weighted avg       0.94      0.94      0.94     13470\n",
      "\n",
      "\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 96.94\n",
      "Confusion Matrix: \n",
      " [[6848  221]\n",
      " [ 191 6210]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      7069\n",
      "           1       0.97      0.97      0.97      6401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     13470\n",
      "   macro avg       0.97      0.97      0.97     13470\n",
      "weighted avg       0.97      0.97      0.97     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 96.94\n",
      "Confusion Matrix: \n",
      " [[6848  221]\n",
      " [ 191 6210]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      7069\n",
      "           1       0.97      0.97      0.97      6401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     13470\n",
      "   macro avg       0.97      0.97      0.97     13470\n",
      "weighted avg       0.97      0.97      0.97     13470\n",
      "\n",
      "\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.6\n",
      "Confusion Matrix: \n",
      " [[7043   26]\n",
      " [  28 6373]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.53\n",
      "Confusion Matrix: \n",
      " [[7034   35]\n",
      " [  28 6373]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 77.82\n",
      "Confusion Matrix: \n",
      " [[5707 1362]\n",
      " [1626 4775]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79      7069\n",
      "           1       0.78      0.75      0.76      6401\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     13470\n",
      "   macro avg       0.78      0.78      0.78     13470\n",
      "weighted avg       0.78      0.78      0.78     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 87.05\n",
      "Confusion Matrix: \n",
      " [[5759 1310]\n",
      " [ 434 5967]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87      7069\n",
      "           1       0.82      0.93      0.87      6401\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     13470\n",
      "   macro avg       0.87      0.87      0.87     13470\n",
      "weighted avg       0.88      0.87      0.87     13470\n",
      "\n",
      "\n",
      "\n",
      " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.65\n",
      "Confusion Matrix: \n",
      " [[7058   11]\n",
      " [  36 6365]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7069\n",
      "           1       1.00      0.99      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.56\n",
      "Confusion Matrix: \n",
      " [[7053   16]\n",
      " [  43 6358]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7069\n",
      "           1       1.00      0.99      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "\n",
      "\n",
      " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.65\n",
      "Confusion Matrix: \n",
      " [[7035   34]\n",
      " [  13 6388]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       0.99      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.65\n",
      "Confusion Matrix: \n",
      " [[7037   32]\n",
      " [  15 6386]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "***********Usng Count Vectorizer****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 97.08\n",
      "Confusion Matrix: \n",
      " [[6970   99]\n",
      " [ 294 6107]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      7069\n",
      "           1       0.98      0.95      0.97      6401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     13470\n",
      "   macro avg       0.97      0.97      0.97     13470\n",
      "weighted avg       0.97      0.97      0.97     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 98.82\n",
      "Confusion Matrix: \n",
      " [[7031   38]\n",
      " [ 121 6280]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      7069\n",
      "           1       0.99      0.98      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n",
      "\n",
      "\n",
      " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "***********Usng Count Vectorizer****************\n",
      "Accuarcy: 99.67\n",
      "Confusion Matrix: \n",
      " [[7039   30]\n",
      " [  15 6386]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n",
      "***********Usng TFIDF Vectorizer****************\n",
      "Accuarcy: 99.66\n",
      "Confusion Matrix: \n",
      " [[7038   31]\n",
      " [  15 6386]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7069\n",
      "           1       1.00      1.00      1.00      6401\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     13470\n",
      "   macro avg       1.00      1.00      1.00     13470\n",
      "weighted avg       1.00      1.00      1.00     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"******USING LEMMATIZED TEXT********\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(lemmatized_text, dataset['Category'], test_size = 0.3, random_state= 0)\n",
    "classifiers = [LogisticRegression(), SGDClassifier(), MultinomialNB(), BernoulliNB(), LinearSVC(),\n",
    "              KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n",
    "               RandomForestClassifier(), XGBClassifier()]\n",
    "for classifier in classifiers:\n",
    "    print(\"\\n\\n\", classifier)\n",
    "    print(\"***********Usng Count Vectorizer****************\")\n",
    "    get_prediction(CountVectorizer(), classifier, X_train, X_test, y_train, y_test)\n",
    "    print(\"***********Usng TFIDF Vectorizer****************\")\n",
    "    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we see that the Decision Tree is giving best with lemmatized texts with accuracy score of 99.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using One hot representation using Stemmed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4451,\n",
       " 2178,\n",
       " 1171,\n",
       " 2430,\n",
       " 3898,\n",
       " 3285,\n",
       " 2055,\n",
       " 3843,\n",
       " 1332,\n",
       " 3379,\n",
       " 3776,\n",
       " 3898,\n",
       " 55,\n",
       " 4451,\n",
       " 2473,\n",
       " 4118,\n",
       " 4578,\n",
       " 1131,\n",
       " 1701,\n",
       " 2748,\n",
       " 508,\n",
       " 4491,\n",
       " 2358,\n",
       " 4432,\n",
       " 4317,\n",
       " 2055,\n",
       " 3776,\n",
       " 1769,\n",
       " 1479,\n",
       " 2178,\n",
       " 3607,\n",
       " 3502,\n",
       " 4580,\n",
       " 7,\n",
       " 2004,\n",
       " 2276,\n",
       " 3898,\n",
       " 4451,\n",
       " 1759,\n",
       " 2265,\n",
       " 532,\n",
       " 2351,\n",
       " 49,\n",
       " 1965,\n",
       " 2748,\n",
       " 3525,\n",
       " 1056,\n",
       " 3773,\n",
       " 3338,\n",
       " 4122,\n",
       " 3119,\n",
       " 3448,\n",
       " 501,\n",
       " 2421,\n",
       " 4985,\n",
       " 3570,\n",
       " 2452,\n",
       " 3119,\n",
       " 2521,\n",
       " 1984,\n",
       " 2808,\n",
       " 3338,\n",
       " 2178,\n",
       " 1171,\n",
       " 1906,\n",
       " 3201,\n",
       " 3832,\n",
       " 3925,\n",
       " 1382,\n",
       " 3628,\n",
       " 3617,\n",
       " 3836,\n",
       " 1760,\n",
       " 3568,\n",
       " 460,\n",
       " 3898,\n",
       " 3448,\n",
       " 3502,\n",
       " 1827,\n",
       " 2473,\n",
       " 1282,\n",
       " 3010,\n",
       " 3,\n",
       " 3898,\n",
       " 3686,\n",
       " 2160,\n",
       " 2178,\n",
       " 1781,\n",
       " 3059,\n",
       " 4122,\n",
       " 2085,\n",
       " 758,\n",
       " 3686,\n",
       " 3862,\n",
       " 1781,\n",
       " 1258,\n",
       " 4082,\n",
       " 2156,\n",
       " 4122,\n",
       " 3242,\n",
       " 1414,\n",
       " 220,\n",
       " 4152,\n",
       " 1677,\n",
       " 4242,\n",
       " 1987,\n",
       " 3087,\n",
       " 606,\n",
       " 3854,\n",
       " 3,\n",
       " 75,\n",
       " 2514,\n",
       " 507,\n",
       " 1082,\n",
       " 3857,\n",
       " 1781,\n",
       " 1258,\n",
       " 4082,\n",
       " 2156,\n",
       " 4122,\n",
       " 776,\n",
       " 532,\n",
       " 1942,\n",
       " 4121,\n",
       " 4800,\n",
       " 3438,\n",
       " 3195,\n",
       " 2475,\n",
       " 3884,\n",
       " 3242,\n",
       " 2085,\n",
       " 1082,\n",
       " 121,\n",
       " 276,\n",
       " 1719,\n",
       " 596,\n",
       " 4491,\n",
       " 714,\n",
       " 776,\n",
       " 2055,\n",
       " 3776,\n",
       " 125,\n",
       " 2081,\n",
       " 3447,\n",
       " 522,\n",
       " 2234,\n",
       " 1688,\n",
       " 3884,\n",
       " 532,\n",
       " 2276,\n",
       " 3898,\n",
       " 4118,\n",
       " 4648,\n",
       " 1264,\n",
       " 2227,\n",
       " 508,\n",
       " 4975,\n",
       " 2358,\n",
       " 2995,\n",
       " 598,\n",
       " 3515,\n",
       " 3338,\n",
       " 2178,\n",
       " 2417,\n",
       " 1381,\n",
       " 4199,\n",
       " 1823,\n",
       " 4199,\n",
       " 2748,\n",
       " 508,\n",
       " 448,\n",
       " 805,\n",
       " 2265,\n",
       " 3474,\n",
       " 2055,\n",
       " 3737,\n",
       " 2085,\n",
       " 4451,\n",
       " 1759,\n",
       " 1734,\n",
       " 1995,\n",
       " 3884,\n",
       " 49,\n",
       " 1995,\n",
       " 3884,\n",
       " 3898,\n",
       " 2358,\n",
       " 4206,\n",
       " 356,\n",
       " 4945,\n",
       " 2877,\n",
       " 307,\n",
       " 4149,\n",
       " 4199,\n",
       " 2670,\n",
       " 2034,\n",
       " 4821,\n",
       " 4975,\n",
       " 2358,\n",
       " 4432,\n",
       " 4231,\n",
       " 3224,\n",
       " 4424,\n",
       " 2174,\n",
       " 2055,\n",
       " 3737,\n",
       " 4206,\n",
       " 2452,\n",
       " 2927,\n",
       " 2808,\n",
       " 830,\n",
       " 3438,\n",
       " 1759,\n",
       " 4506,\n",
       " 3857,\n",
       " 4491,\n",
       " 515,\n",
       " 515,\n",
       " 1823,\n",
       " 485,\n",
       " 1995,\n",
       " 3884,\n",
       " 3898,\n",
       " 3873,\n",
       " 2358,\n",
       " 4829,\n",
       " 2329,\n",
       " 4451,\n",
       " 2358,\n",
       " 2995,\n",
       " 1823,\n",
       " 3919,\n",
       " 2747,\n",
       " 3508,\n",
       " 4183,\n",
       " 3438,\n",
       " 2216,\n",
       " 2102,\n",
       " 1653,\n",
       " 758,\n",
       " 1414,\n",
       " 2358,\n",
       " 4206,\n",
       " 3122,\n",
       " 3903,\n",
       " 532,\n",
       " 1356,\n",
       " 378,\n",
       " 4656,\n",
       " 4491,\n",
       " 2859,\n",
       " 4980,\n",
       " 2088,\n",
       " 2227,\n",
       " 3605,\n",
       " 4317,\n",
       " 356,\n",
       " 113,\n",
       " 3898,\n",
       " 4317,\n",
       " 3898,\n",
       " 2469,\n",
       " 4980,\n",
       " 3242,\n",
       " 563,\n",
       " 4444,\n",
       " 2683,\n",
       " 3438,\n",
       " 4049,\n",
       " 4099,\n",
       " 1785,\n",
       " 3087,\n",
       " 1007,\n",
       " 2968,\n",
       " 4378,\n",
       " 3428,\n",
       " 1794,\n",
       " 3242,\n",
       " 2399,\n",
       " 1026,\n",
       " 4049,\n",
       " 982,\n",
       " 2085,\n",
       " 825,\n",
       " 1653,\n",
       " 1216,\n",
       " 1264,\n",
       " 1137,\n",
       " 1082,\n",
       " 1500,\n",
       " 3898,\n",
       " 356,\n",
       " 1984,\n",
       " 4491,\n",
       " 2358,\n",
       " 2995,\n",
       " 3448,\n",
       " 4122,\n",
       " 4432,\n",
       " 3772,\n",
       " 3242,\n",
       " 4627,\n",
       " 3438,\n",
       " 3898,\n",
       " 1222,\n",
       " 4890,\n",
       " 4078,\n",
       " 4816,\n",
       " 1931,\n",
       " 4118,\n",
       " 2085,\n",
       " 276,\n",
       " 4059,\n",
       " 2178,\n",
       " 3232,\n",
       " 596,\n",
       " 2955,\n",
       " 2085,\n",
       " 920,\n",
       " 3597,\n",
       " 1931,\n",
       " 3898,\n",
       " 1585,\n",
       " 1827,\n",
       " 397,\n",
       " 2156,\n",
       " 1258,\n",
       " 4082,\n",
       " 3242,\n",
       " 3772,\n",
       " 4122,\n",
       " 3160,\n",
       " 3832,\n",
       " 333,\n",
       " 2234,\n",
       " 4624,\n",
       " 2242,\n",
       " 1249,\n",
       " 651,\n",
       " 3,\n",
       " 4101,\n",
       " 4844,\n",
       " 989,\n",
       " 254,\n",
       " 816,\n",
       " 197,\n",
       " 868,\n",
       " 51,\n",
       " 2766,\n",
       " 1132,\n",
       " 3242,\n",
       " 3854,\n",
       " 653,\n",
       " 3925,\n",
       " 4760,\n",
       " 2996,\n",
       " 478,\n",
       " 134,\n",
       " 1282,\n",
       " 3884,\n",
       " 3122,\n",
       " 3050,\n",
       " 3892,\n",
       " 3686,\n",
       " 2623,\n",
       " 1927,\n",
       " 3942,\n",
       " 2028,\n",
       " 898,\n",
       " 3925,\n",
       " 79,\n",
       " 1007,\n",
       " 2383,\n",
       " 3203,\n",
       " 1521,\n",
       " 333,\n",
       " 1759,\n",
       " 822,\n",
       " 3209,\n",
       " 873,\n",
       " 49,\n",
       " 4220,\n",
       " 3201,\n",
       " 3832,\n",
       " 1382,\n",
       " 967,\n",
       " 898,\n",
       " 2623,\n",
       " 276,\n",
       " 1132,\n",
       " 2840,\n",
       " 3884,\n",
       " 2452,\n",
       " 3,\n",
       " 2586,\n",
       " 331,\n",
       " 3836,\n",
       " 3114,\n",
       " 2044,\n",
       " 3832,\n",
       " 711,\n",
       " 4628,\n",
       " 4198,\n",
       " 840,\n",
       " 3,\n",
       " 3898,\n",
       " 3114,\n",
       " 1680,\n",
       " 472,\n",
       " 3438,\n",
       " 3884,\n",
       " 3,\n",
       " 758,\n",
       " 2842,\n",
       " 331,\n",
       " 1769,\n",
       " 935,\n",
       " 3898,\n",
       " 211,\n",
       " 3183,\n",
       " 4328,\n",
       " 3686,\n",
       " 499,\n",
       " 2586,\n",
       " 3438,\n",
       " 2808,\n",
       " 2937,\n",
       " 2586,\n",
       " 4829,\n",
       " 613,\n",
       " 935,\n",
       " 4770,\n",
       " 3966,\n",
       " 1927,\n",
       " 2192,\n",
       " 864,\n",
       " 4829,\n",
       " 1315,\n",
       " 1030,\n",
       " 2937,\n",
       " 4350,\n",
       " 3,\n",
       " 75,\n",
       " 1931,\n",
       " 4979,\n",
       " 4118,\n",
       " 2586]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = 5000\n",
    "onehot_stemmed_text = [one_hot(word, voc_size) for word in stemmed_text]\n",
    "print(len(onehot_stemmed_text))\n",
    "onehot_stemmed_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3773 3338 4122 ... 4979 4118 2586]\n",
      " [   0    0    0 ...  658  749  591]\n",
      " [   0    0    0 ... 4100 4400 3884]\n",
      " ...\n",
      " [ 107 2513 2382 ... 3427   69  772]\n",
      " [   0    0    0 ... 3427 4777 3162]\n",
      " [4585 3359 1641 ... 3427 1881  772]]\n"
     ]
    }
   ],
   "source": [
    "sent_length = 400\n",
    "embedded_text = pad_sequences(onehot_stemmed_text, padding='pre', maxlen=sent_length)\n",
    "print(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 600)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               280400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,280,501\n",
      "Trainable params: 3,280,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features = 600\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
    "model1.add(LSTM(100))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.array(embedded_text)\n",
    "y_final = dataset['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44898, 400), (44898,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 31428 samples, validate on 13470 samples\n",
      "Epoch 1/20\n",
      "31428/31428 [==============================] - 399s 13ms/step - loss: 0.1807 - acc: 0.9365 - val_loss: 0.0675 - val_acc: 0.9788\n",
      "Epoch 2/20\n",
      "31428/31428 [==============================] - 397s 13ms/step - loss: 0.0572 - acc: 0.9820 - val_loss: 0.0915 - val_acc: 0.9638\n",
      "Epoch 3/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0512 - acc: 0.9838 - val_loss: 0.0595 - val_acc: 0.9826\n",
      "Epoch 4/20\n",
      "31428/31428 [==============================] - 396s 13ms/step - loss: 0.1248 - acc: 0.9639 - val_loss: 0.1457 - val_acc: 0.9366\n",
      "Epoch 5/20\n",
      "31428/31428 [==============================] - 396s 13ms/step - loss: 0.0764 - acc: 0.9727 - val_loss: 0.0891 - val_acc: 0.9712\n",
      "Epoch 6/20\n",
      "31428/31428 [==============================] - 396s 13ms/step - loss: 0.0582 - acc: 0.9804 - val_loss: 0.0523 - val_acc: 0.9839\n",
      "Epoch 7/20\n",
      "31428/31428 [==============================] - 405s 13ms/step - loss: 0.0387 - acc: 0.9878 - val_loss: 0.0558 - val_acc: 0.9842\n",
      "Epoch 8/20\n",
      "31428/31428 [==============================] - 396s 13ms/step - loss: 0.0947 - acc: 0.9697 - val_loss: 0.0724 - val_acc: 0.9775\n",
      "Epoch 9/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0711 - val_acc: 0.9777\n",
      "Epoch 10/20\n",
      "31428/31428 [==============================] - 394s 13ms/step - loss: 0.0772 - acc: 0.9717 - val_loss: 0.0867 - val_acc: 0.9707\n",
      "Epoch 11/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0381 - acc: 0.9892 - val_loss: 0.0441 - val_acc: 0.9857\n",
      "Epoch 12/20\n",
      "31428/31428 [==============================] - 394s 13ms/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0386 - val_acc: 0.9869\n",
      "Epoch 13/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0108 - acc: 0.9973 - val_loss: 0.0320 - val_acc: 0.9901\n",
      "Epoch 14/20\n",
      "31428/31428 [==============================] - 397s 13ms/step - loss: 0.0061 - acc: 0.9989 - val_loss: 0.0329 - val_acc: 0.9912\n",
      "Epoch 15/20\n",
      "31428/31428 [==============================] - 398s 13ms/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.0321 - val_acc: 0.9912\n",
      "Epoch 16/20\n",
      "31428/31428 [==============================] - 407s 13ms/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.0337 - val_acc: 0.9915\n",
      "Epoch 17/20\n",
      "31428/31428 [==============================] - 397s 13ms/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0332 - val_acc: 0.9918\n",
      "Epoch 18/20\n",
      "31428/31428 [==============================] - 396s 13ms/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0345 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0023 - acc: 0.9997 - val_loss: 0.0344 - val_acc: 0.9916\n",
      "Epoch 20/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.0343 - val_acc: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e711f5128>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 99.17\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1.predict_classes(X_test)\n",
    "print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[7025   44]\n",
      " [  68 6333]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using One hot representation using Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4451,\n",
       " 2178,\n",
       " 1171,\n",
       " 2430,\n",
       " 3898,\n",
       " 3285,\n",
       " 2055,\n",
       " 3843,\n",
       " 2169,\n",
       " 3379,\n",
       " 2206,\n",
       " 3898,\n",
       " 55,\n",
       " 4451,\n",
       " 2473,\n",
       " 1556,\n",
       " 4578,\n",
       " 1131,\n",
       " 52,\n",
       " 4507,\n",
       " 508,\n",
       " 4491,\n",
       " 2358,\n",
       " 4432,\n",
       " 1260,\n",
       " 2055,\n",
       " 2206,\n",
       " 1769,\n",
       " 3235,\n",
       " 2178,\n",
       " 3607,\n",
       " 1279,\n",
       " 4580,\n",
       " 7,\n",
       " 2004,\n",
       " 2276,\n",
       " 3898,\n",
       " 4451,\n",
       " 1111,\n",
       " 2265,\n",
       " 532,\n",
       " 742,\n",
       " 1626,\n",
       " 1965,\n",
       " 2748,\n",
       " 3525,\n",
       " 1056,\n",
       " 3773,\n",
       " 1222,\n",
       " 855,\n",
       " 4709,\n",
       " 1311,\n",
       " 2257,\n",
       " 1577,\n",
       " 4985,\n",
       " 3570,\n",
       " 2452,\n",
       " 4709,\n",
       " 2521,\n",
       " 857,\n",
       " 3911,\n",
       " 1222,\n",
       " 2178,\n",
       " 1171,\n",
       " 2806,\n",
       " 4356,\n",
       " 147,\n",
       " 4611,\n",
       " 2701,\n",
       " 3628,\n",
       " 4594,\n",
       " 825,\n",
       " 3745,\n",
       " 3568,\n",
       " 460,\n",
       " 3898,\n",
       " 3448,\n",
       " 3502,\n",
       " 1827,\n",
       " 2473,\n",
       " 3957,\n",
       " 3010,\n",
       " 3,\n",
       " 3898,\n",
       " 3686,\n",
       " 2160,\n",
       " 2178,\n",
       " 936,\n",
       " 496,\n",
       " 855,\n",
       " 2085,\n",
       " 758,\n",
       " 3686,\n",
       " 843,\n",
       " 936,\n",
       " 1258,\n",
       " 190,\n",
       " 803,\n",
       " 855,\n",
       " 3242,\n",
       " 1414,\n",
       " 4180,\n",
       " 2084,\n",
       " 1677,\n",
       " 275,\n",
       " 1987,\n",
       " 3087,\n",
       " 4127,\n",
       " 976,\n",
       " 3,\n",
       " 3302,\n",
       " 3930,\n",
       " 4843,\n",
       " 1082,\n",
       " 2861,\n",
       " 936,\n",
       " 1258,\n",
       " 190,\n",
       " 803,\n",
       " 855,\n",
       " 776,\n",
       " 532,\n",
       " 1942,\n",
       " 4121,\n",
       " 664,\n",
       " 2361,\n",
       " 3195,\n",
       " 4869,\n",
       " 3884,\n",
       " 3242,\n",
       " 2085,\n",
       " 4206,\n",
       " 121,\n",
       " 276,\n",
       " 1719,\n",
       " 3254,\n",
       " 4491,\n",
       " 4616,\n",
       " 776,\n",
       " 2055,\n",
       " 2206,\n",
       " 125,\n",
       " 4965,\n",
       " 1448,\n",
       " 522,\n",
       " 786,\n",
       " 1688,\n",
       " 3884,\n",
       " 532,\n",
       " 2276,\n",
       " 3898,\n",
       " 1556,\n",
       " 4648,\n",
       " 942,\n",
       " 3426,\n",
       " 508,\n",
       " 4170,\n",
       " 2358,\n",
       " 2995,\n",
       " 821,\n",
       " 3515,\n",
       " 1222,\n",
       " 2178,\n",
       " 2417,\n",
       " 1381,\n",
       " 4199,\n",
       " 1823,\n",
       " 4199,\n",
       " 4507,\n",
       " 508,\n",
       " 657,\n",
       " 805,\n",
       " 2265,\n",
       " 3474,\n",
       " 2055,\n",
       " 4020,\n",
       " 2441,\n",
       " 4451,\n",
       " 1111,\n",
       " 1734,\n",
       " 1995,\n",
       " 3884,\n",
       " 1626,\n",
       " 1995,\n",
       " 3884,\n",
       " 3898,\n",
       " 2358,\n",
       " 4206,\n",
       " 356,\n",
       " 4925,\n",
       " 257,\n",
       " 307,\n",
       " 4149,\n",
       " 4199,\n",
       " 2670,\n",
       " 1307,\n",
       " 4535,\n",
       " 4663,\n",
       " 2358,\n",
       " 4432,\n",
       " 1698,\n",
       " 3224,\n",
       " 4424,\n",
       " 2174,\n",
       " 2820,\n",
       " 1632,\n",
       " 4206,\n",
       " 2452,\n",
       " 2927,\n",
       " 1259,\n",
       " 1413,\n",
       " 2361,\n",
       " 1111,\n",
       " 4506,\n",
       " 2861,\n",
       " 190,\n",
       " 2092,\n",
       " 2092,\n",
       " 1823,\n",
       " 485,\n",
       " 1995,\n",
       " 3884,\n",
       " 3898,\n",
       " 3873,\n",
       " 2358,\n",
       " 95,\n",
       " 2329,\n",
       " 4451,\n",
       " 2358,\n",
       " 2995,\n",
       " 1823,\n",
       " 3919,\n",
       " 3735,\n",
       " 3508,\n",
       " 4183,\n",
       " 2361,\n",
       " 2216,\n",
       " 2102,\n",
       " 1653,\n",
       " 758,\n",
       " 1429,\n",
       " 2358,\n",
       " 4206,\n",
       " 944,\n",
       " 3903,\n",
       " 532,\n",
       " 2152,\n",
       " 378,\n",
       " 4656,\n",
       " 4491,\n",
       " 561,\n",
       " 4438,\n",
       " 2088,\n",
       " 3426,\n",
       " 3605,\n",
       " 4317,\n",
       " 356,\n",
       " 113,\n",
       " 3898,\n",
       " 1169,\n",
       " 3898,\n",
       " 4258,\n",
       " 4438,\n",
       " 3242,\n",
       " 563,\n",
       " 4444,\n",
       " 2683,\n",
       " 1391,\n",
       " 4632,\n",
       " 4236,\n",
       " 1785,\n",
       " 3087,\n",
       " 4638,\n",
       " 1570,\n",
       " 4378,\n",
       " 2443,\n",
       " 1794,\n",
       " 3242,\n",
       " 3182,\n",
       " 1026,\n",
       " 4049,\n",
       " 1498,\n",
       " 2085,\n",
       " 306,\n",
       " 1653,\n",
       " 4746,\n",
       " 942,\n",
       " 1137,\n",
       " 4206,\n",
       " 4714,\n",
       " 3898,\n",
       " 356,\n",
       " 2727,\n",
       " 4491,\n",
       " 2358,\n",
       " 2995,\n",
       " 707,\n",
       " 855,\n",
       " 4432,\n",
       " 3772,\n",
       " 3242,\n",
       " 4627,\n",
       " 2361,\n",
       " 3898,\n",
       " 1222,\n",
       " 4890,\n",
       " 4078,\n",
       " 4816,\n",
       " 1967,\n",
       " 4118,\n",
       " 2085,\n",
       " 4306,\n",
       " 1356,\n",
       " 2178,\n",
       " 3232,\n",
       " 3254,\n",
       " 2955,\n",
       " 2085,\n",
       " 920,\n",
       " 1500,\n",
       " 1967,\n",
       " 3898,\n",
       " 4652,\n",
       " 1827,\n",
       " 397,\n",
       " 803,\n",
       " 1258,\n",
       " 190,\n",
       " 3242,\n",
       " 3772,\n",
       " 855,\n",
       " 1480,\n",
       " 147,\n",
       " 333,\n",
       " 786,\n",
       " 4624,\n",
       " 3912,\n",
       " 1659,\n",
       " 4872,\n",
       " 3,\n",
       " 192,\n",
       " 4844,\n",
       " 989,\n",
       " 2194,\n",
       " 816,\n",
       " 4560,\n",
       " 868,\n",
       " 51,\n",
       " 3113,\n",
       " 1132,\n",
       " 3242,\n",
       " 2228,\n",
       " 653,\n",
       " 1092,\n",
       " 3993,\n",
       " 2589,\n",
       " 478,\n",
       " 134,\n",
       " 3957,\n",
       " 3884,\n",
       " 3122,\n",
       " 3050,\n",
       " 4224,\n",
       " 3686,\n",
       " 4406,\n",
       " 3556,\n",
       " 3942,\n",
       " 2028,\n",
       " 898,\n",
       " 4611,\n",
       " 79,\n",
       " 4833,\n",
       " 3529,\n",
       " 4452,\n",
       " 1521,\n",
       " 333,\n",
       " 1111,\n",
       " 1103,\n",
       " 992,\n",
       " 873,\n",
       " 1626,\n",
       " 4220,\n",
       " 4200,\n",
       " 147,\n",
       " 2701,\n",
       " 803,\n",
       " 898,\n",
       " 4406,\n",
       " 276,\n",
       " 1132,\n",
       " 2840,\n",
       " 3884,\n",
       " 2452,\n",
       " 3,\n",
       " 577,\n",
       " 331,\n",
       " 825,\n",
       " 3114,\n",
       " 1370,\n",
       " 147,\n",
       " 2895,\n",
       " 4628,\n",
       " 3678,\n",
       " 840,\n",
       " 3,\n",
       " 3898,\n",
       " 3114,\n",
       " 1680,\n",
       " 472,\n",
       " 2361,\n",
       " 3884,\n",
       " 3,\n",
       " 758,\n",
       " 53,\n",
       " 331,\n",
       " 1769,\n",
       " 935,\n",
       " 3898,\n",
       " 211,\n",
       " 3183,\n",
       " 4328,\n",
       " 3686,\n",
       " 3086,\n",
       " 2586,\n",
       " 2361,\n",
       " 1259,\n",
       " 2937,\n",
       " 2586,\n",
       " 95,\n",
       " 2198,\n",
       " 935,\n",
       " 2982,\n",
       " 3966,\n",
       " 1927,\n",
       " 3784,\n",
       " 864,\n",
       " 95,\n",
       " 1315,\n",
       " 1323,\n",
       " 2937,\n",
       " 825,\n",
       " 3,\n",
       " 3302,\n",
       " 1967,\n",
       " 4979,\n",
       " 1556,\n",
       " 2586]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = 5000\n",
    "onehot_lemmatized_text = [one_hot(word, voc_size) for word in lemmatized_text]\n",
    "print(len(onehot_lemmatized_text))\n",
    "onehot_lemmatized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3773 1222  855 ... 4979 1556 2586]\n",
      " [   0    0    0 ... 4359  749 1032]\n",
      " [   0    0    0 ... 4451 1400 3884]\n",
      " ...\n",
      " [ 107 2513 2382 ... 3427   69  772]\n",
      " [   0    0    0 ... 3427 4777 3162]\n",
      " [3575 3359 1641 ... 3427 1881  772]]\n"
     ]
    }
   ],
   "source": [
    "sent_length = 400\n",
    "embedded_text = pad_sequences(onehot_lemmatized_text, padding='pre', maxlen=sent_length)\n",
    "print(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 600)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               280400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,280,501\n",
      "Trainable params: 3,280,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features = 600\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.array(embedded_text)\n",
    "y_final = dataset['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31428 samples, validate on 13470 samples\n",
      "Epoch 1/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.1704 - acc: 0.9383 - val_loss: 0.0702 - val_acc: 0.9763\n",
      "Epoch 2/20\n",
      "31428/31428 [==============================] - 393s 12ms/step - loss: 0.1042 - acc: 0.9597 - val_loss: 0.0752 - val_acc: 0.9773\n",
      "Epoch 3/20\n",
      "31428/31428 [==============================] - 393s 13ms/step - loss: 0.0578 - acc: 0.9813 - val_loss: 0.0567 - val_acc: 0.9828\n",
      "Epoch 4/20\n",
      "31428/31428 [==============================] - 392s 12ms/step - loss: 0.0355 - acc: 0.9886 - val_loss: 0.1203 - val_acc: 0.9554\n",
      "Epoch 5/20\n",
      "31428/31428 [==============================] - 402s 13ms/step - loss: 0.0374 - acc: 0.9875 - val_loss: 0.0454 - val_acc: 0.9860\n",
      "Epoch 6/20\n",
      "31428/31428 [==============================] - 391s 12ms/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0425 - val_acc: 0.9863\n",
      "Epoch 7/20\n",
      "31428/31428 [==============================] - 392s 12ms/step - loss: 0.0139 - acc: 0.9962 - val_loss: 0.0443 - val_acc: 0.9869\n",
      "Epoch 8/20\n",
      "31428/31428 [==============================] - 393s 13ms/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0446 - val_acc: 0.9889\n",
      "Epoch 9/20\n",
      "31428/31428 [==============================] - 393s 12ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.0433 - val_acc: 0.9883\n",
      "Epoch 10/20\n",
      "31428/31428 [==============================] - 394s 13ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0422 - val_acc: 0.9904\n",
      "Epoch 11/20\n",
      "31428/31428 [==============================] - 394s 13ms/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0522 - val_acc: 0.9877\n",
      "Epoch 12/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0405 - val_acc: 0.9902\n",
      "Epoch 13/20\n",
      "31428/31428 [==============================] - 393s 13ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0512 - val_acc: 0.9875\n",
      "Epoch 14/20\n",
      "31428/31428 [==============================] - 402s 13ms/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0859 - val_acc: 0.9762\n",
      "Epoch 15/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0894 - acc: 0.9702 - val_loss: 0.0509 - val_acc: 0.9831\n",
      "Epoch 16/20\n",
      "31428/31428 [==============================] - 393s 13ms/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0455 - val_acc: 0.9859\n",
      "Epoch 17/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.0441 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/20\n",
      "31428/31428 [==============================] - 394s 13ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0440 - val_acc: 0.9888\n",
      "Epoch 19/20\n",
      "31428/31428 [==============================] - 393s 13ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0426 - val_acc: 0.9892\n",
      "Epoch 20/20\n",
      "31428/31428 [==============================] - 395s 13ms/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0426 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e24606ba8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy: 98.93\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict_classes(X_test)\n",
    "print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[7010   59]\n",
      " [  85 6316]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7069\n",
      "           1       0.99      0.99      0.99      6401\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     13470\n",
      "   macro avg       0.99      0.99      0.99     13470\n",
      "weighted avg       0.99      0.99      0.99     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.array(final_text_result)\n",
    "y_final = dataset['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
